<!doctype html>
<html lang="es">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>ML5 01 · Cámara + Detección</title>

  <!-- ml5.js (versión fija para evitar cambios de API) -->
  <script src="https://unpkg.com/ml5@0.12.2/dist/ml5.min.js"></script>

  <style>
    body { font-family: system-ui, Arial, sans-serif; margin: 16px; }
    #wrap { position: relative; width: min(720px, 100%); }
    video, canvas { width: 100%; height: auto; border-radius: 10px; }
    canvas { position: absolute; left: 0; top: 0; }
    button { padding: 10px 12px; font-size: 14px; cursor: pointer; }
    .row { display:flex; gap:12px; flex-wrap: wrap; align-items:center; margin-bottom: 10px; }
    .badge { display:inline-block; border:1px solid #ccc; border-radius:999px; padding: 3px 10px; }
  </style>
</head>

<body>
  <h1>ML5 01 · Cámara en móvil + detección</h1>

  <div class="row">
    <button id="btnStart">Iniciar cámara</button>
    <span id="status" class="badge">Estado: esperando…</span>
    <span id="best" class="badge">Mejor detección: —</span>
  </div>

  <div id="wrap">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="overlay"></canvas>
  </div>

  <div>
   <h3> Explicación de cómo funciona el código</h3>

   <p> El archivo HTML tiene una estructura sencilla que incluye un botón, un video y un canvas (lienzo) donde se mostrarán las detecciones.

El botón "Iniciar cámara" permite activar la cámara del dispositivo.

El video muestra lo que capta la cámara.

El canvas es una especie de superficie en blanco donde se dibujan las cajas alrededor de los objetos que el modelo de inteligencia artificial detecta.

La función initCamera() se encarga de pedir permiso para usar la cámara del dispositivo. Al hacer clic en el botón, el navegador muestra un mensaje pidiendo permiso. Si el usuario lo acepta, se muestra el video de la cámara en la página.

Se usa navigator.mediaDevices.getUserMedia() para obtener el video desde la cámara.

Una vez que la cámara está activa, se carga un modelo de IA llamado COCO-SSD. Este modelo puede identificar varios objetos como personas, perros, coches, etc.

El modelo se carga con ml5.objectDetector("cocossd"). Este es un modelo preentrenado que ya sabe cómo identificar una lista de objetos comunes.

La función loopDetect() se ejecuta constantemente para revisar lo que está viendo la cámara.

Cada vez que se detecta algo, el modelo devuelve una lista con los objetos encontrados. Para cada objeto, dibuja una caja alrededor de él en el canvas (el lienzo).

También muestra una etiqueta con el nombre del objeto y el porcentaje de confianza que tiene el modelo en la detección (por ejemplo, "persona (85%)").

La función draw() se encarga de dibujar esas cajas y etiquetas en el canvas. Cada objeto detectado recibe un cuadro de color y su nombre con el porcentaje de confianza al lado.

También se guarda el objeto con la mayor confianza en la variable bestEl para mostrar cuál es la mejor detección en ese momento.
En los navegadores, por razones de seguridad, se requiere que el usuario haga una acción (como pulsar un botón) para activar la cámara. Esto evita que los sitios web puedan acceder a la cámara sin que el usuario lo sepa. </p>

  </div>


  <script>
    const btnStart = document.getElementById("btnStart");
    const statusEl = document.getElementById("status");
    const bestEl = document.getElementById("best");

    const video = document.getElementById("video");
    const canvas = document.getElementById("overlay");
    const ctx = canvas.getContext("2d");

    let detector = null;
    let running = false;

    function setStatus(t) { statusEl.textContent = "Estado: " + t; }

    async function initCamera() {
      // En móvil, "environment" intenta usar la cámara trasera
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: { ideal: "environment" } },
        audio: false
      });

      video.srcObject = stream;

      await new Promise(res => video.onloadedmetadata = () => res());

      // Ajustar canvas al tamaño real del vídeo
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
    }

    function draw(detections) {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.lineWidth = 3;
      ctx.font = "16px system-ui, Arial";

      for (const d of detections) {
        ctx.strokeRect(d.x, d.y, d.width, d.height);
        const label = `${d.label} (${Math.round(d.confidence * 100)}%)`;
        ctx.fillText(label, d.x + 6, Math.max(16, d.y + 16));
      }
    }

    function pickBest(detections) {
      if (!detections || detections.length === 0) return null;
      let best = detections[0];
      for (const d of detections) {
        if (d.confidence > best.confidence) best = d;
      }
      return best;
    }

    function loopDetect() {
      if (!running || !detector) return;

      detector.detect(video, (err, results) => {
        if (err) {
          console.error(err);
          setStatus("error en detección (ver consola)");
          running = false;
          return;
        }

        draw(results);

        const best = pickBest(results);
        bestEl.textContent = best
          ? `Mejor detección: ${best.label} (${Math.round(best.confidence * 100)}%)`
          : "Mejor detección: —";

        requestAnimationFrame(loopDetect);
      });
    }

    btnStart.addEventListener("click", async () => {
      try {
        btnStart.disabled = true;

        setStatus("iniciando cámara…");
        await initCamera();

        setStatus("cargando modelo COCO-SSD…");
        detector = await ml5.objectDetector("cocossd");

        setStatus("detectando…");
        running = true;
        loopDetect();

      } catch (e) {
        console.error(e);
        setStatus(`error: ${e.name} — ${e.message}`);
        btnStart.disabled = false;
      }
    });
  </script>
</body>
</html>
